{
    "cells": [{
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/DanielRondonGarcia/LSC/blob/main/Model_CNN_Preentreno_Funcional12_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "  pip install wandb gputil pycm scikit-learn ipython Pillow tensorflow mlxtend pandas opencv-python tqdm torch seaborn scikit-learn scipy"
            ],
            "metadata": {
                "id": "8wPqZ9bvFkhm",
                "outputId": "a6029906-de67-4213-dc15-a14d9037a70b",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                }
            },
            "execution_count": 5,
            "outputs": [{
                "output_type": "stream",
                "name": "stdout",
                "text": [
                    "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n"
                ]
            }]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [{
                "name": "stderr",
                "output_type": "stream",
                "text": [
                    "C:\\Users\\AnonimusXD\\AppData\\Local\\Temp\\ipykernel_21816\\3275798153.py:9: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
                    "  from IPython.core.display import display, HTML\n",
                    "c:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                    "  from .autonotebook import tqdm as notebook_tqdm\n"
                ]
            }],
            "source": [
                "# Import libreries\n",
                "import zipfile\n",
                "import tarfile\n",
                "from six.moves import urllib\n",
                "from pycm import *\n",
                "import GPUtil\n",
                "import logging\n",
                "import sklearn\n",
                "from IPython.core.display import display, HTML\n",
                "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix, ConfusionMatrixDisplay\n",
                "from PIL import Image\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import backend as K\n",
                "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Flatten\n",
                "from tensorflow.keras.optimizers import Adam, Adamax\n",
                "from tensorflow.keras.metrics import categorical_crossentropy\n",
                "from tensorflow.keras import regularizers\n",
                "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
                "from tensorflow.keras.models import Model, load_model, Sequential, save_model\n",
                "from tensorflow.keras.utils import plot_model\n",
                "from tensorflow.keras.backend import clear_session\n",
                "from keras.preprocessing.image import ImageDataGenerator, load_img, image, img_to_array\n",
                "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback\n",
                "from sklearn.preprocessing import LabelBinarizer\n",
                "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from mlxtend.plotting import plot_decision_regions\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import datetime\n",
                "import shutil\n",
                "import time\n",
                "import cv2 as cv2\n",
                "from tqdm import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.pyplot import imshow\n",
                "import os\n",
                "import torch\n",
                "import seaborn as sns\n",
                "import wandb\n",
                "sns.set_style('darkgrid')\n",
                "# EXTRAS PRUEBAS\n",
                "# stop annoying tensorflow warning messages\n",
                "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
                "%load_ext tensorboard\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [{
                "name": "stdout",
                "output_type": "stream",
                "text": [
                    "False\n",
                    "Num GPUs: 1\n"
                ]
            }],
            "source": [
                "\n",
                "\"\"\" config = tf.compat.v1.ConfigProto()\n",
                "config.gpu_options.allow_growth = True\n",
                "sess = tf.compat.v1.Session(config=config) \"\"\"\n",
                "use_cuda = torch.cuda.is_available()\n",
                "\n",
                "\n",
                "print(torch.cuda.is_available())\n",
                "physical_devices = tf.config.list_physical_devices('GPU')\n",
                "print(\"Num GPUs:\", len(physical_devices))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [{
                "name": "stdout",
                "output_type": "stream",
                "text": [
                    "| ID | GPU | MEM |\n",
                    "------------------\n",
                    "|  0 | 41% | 25% |\n"
                ]
            }],
            "source": [
                "GPUtil.showUtilization()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "K.clear_session()\n",
                "data_dir = './Todas'\n",
                "path_master = './saved_model'\n",
                "path_master_exist = os.path.isdir(path_master)\n",
                "\n",
                "data_balance = True  # Traer CSV con paths de imagenes balanceadas\n",
                "\n",
                "if path_master_exist:\n",
                "    print('La carpeta saved_model existe.')\n",
                "else:\n",
                "    os.mkdir(path_master)\n",
                "exist_csv_with_split = os.path.isdir(data_dir)\n",
                "if not exist_csv_with_split:\n",
                "    DOWNLOAD_URL = 'https://github.com/Kandriws/LSC/raw/main/Comprimido/Todas.tar'\n",
                "    print('Downloading LSC images from %s...' % DOWNLOAD_URL)\n",
                "    urllib.request.urlretrieve(DOWNLOAD_URL, 'Todas.tar')\n",
                "    file_untar = 'Todas.tar'\n",
                "    untar = tarfile.TarFile(file_untar)\n",
                "    untar.extractall()\n",
                "    untar.close()\n",
                "    print('LSC photos are located in %s' % data_dir)\n",
                "\n",
                "split_test_data_exist = os.path.isfile(path_master+'/split_test_data.csv')\n",
                "split_train_data_exist = os.path.isfile(path_master+'/split_train_data.csv')\n",
                "split_val_data_exist = os.path.isfile(path_master+'/split_val_data.csv')\n",
                "if data_balance:\n",
                "    if not split_test_data_exist:\n",
                "        DOWNLOAD_URL = 'https://raw.githubusercontent.com/Kandriws/LSC/main/saved_model/split_test_data.csv'\n",
                "        urllib.request.urlretrieve(\n",
                "            DOWNLOAD_URL, path_master+'/split_test_data.csv')\n",
                "    if not split_train_data_exist:\n",
                "        DOWNLOAD_URL = 'https://raw.githubusercontent.com/Kandriws/LSC/main/saved_model/split_train_data.csv'\n",
                "        urllib.request.urlretrieve(\n",
                "            DOWNLOAD_URL, path_master+'/split_train_data.csv')\n",
                "    if not split_val_data_exist:\n",
                "        DOWNLOAD_URL = 'https://raw.githubusercontent.com/Kandriws/LSC/main/saved_model/split_val_data.csv'\n",
                "        urllib.request.urlretrieve(\n",
                "            DOWNLOAD_URL, path_master+'/split_val_data.csv')\n",
                "\n",
                "\n",
                "\"\"\" HyperParametros \"\"\"\n",
                "create_filter = False\n",
                "epoch = 100\n",
                "width, height = 224, 224\n",
                "channels = 3\n",
                "batch_size = 5\n",
                "img_shape = (height, width, channels)\n",
                "img_size = (width, height)\n",
                "fil_conv_1 = 32\n",
                "fil_conv_2 = 64\n",
                "tamano_fil_conv_1 = (3, 3)\n",
                "tamano_fil_conv_2 = (2, 2)\n",
                "tamano_pool = (2, 2)\n",
                "lr = 0.001\n",
                "model_name = 'EfficientNetV2S'  # EfficientNetV2S\n",
                "datestring = datetime.datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
                "path_file = './saved_model/'+model_name+'-'+str(epoch)+'-'+datestring\n",
                "csv_name_split_train = 'split_train_data.csv'\n",
                "csv_name_split_val = 'split_val_data.csv'\n",
                "csv_name_split_test = 'split_test_data.csv'\n",
                "flag = True  # para balancear el dataset creando imagenes sinteticas False = inicializa, True = impide\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "def scalar(img):\n",
                "    return img\n",
                "\n",
                "\n",
                "def processImage(image):\n",
                "    image = cv2.imread(image)\n",
                "    \"\"\" image = cv2.imread(image)\n",
                "    blurred = cv2.GaussianBlur(image, (3, 3), 0)\n",
                "    gray = cv2.cv2tColor(blurred, cv2.COLOR_BGR2GRAY)\n",
                "    grad_x = cv2.Sobel(gray, cv2.CV2_16SC1, 1, 0)\n",
                "    # Encuentra el gradiente en la dirección y\n",
                "    grad_y = cv2.Sobel(gray, cv2.CV_16SC1, 0, 1)\n",
                "    edge1 = cv2.Canny(grad_x, grad_y, 10, 100) \"\"\"\n",
                "\n",
                "    return image\n",
                "\n",
                "\n",
                "train_df_datagen = ImageDataGenerator(\n",
                "    preprocessing_function=scalar,\n",
                "    horizontal_flip=True,\n",
                "    featurewise_center=True,\n",
                "    featurewise_std_normalization=True,\n",
                "    rotation_range=20,\n",
                "    width_shift_range=0.2,\n",
                "    height_shift_range=0.2,\n",
                "    validation_split=0.2\n",
                ")\n",
                "test_df_datagen = ImageDataGenerator(\n",
                "    preprocessing_function=scalar,\n",
                ")\n",
                "valid_df_datagen = ImageDataGenerator(\n",
                "    preprocessing_function=scalar,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_filter_canny(fpath_img, i):\n",
                "    img = cv2.imread(fpath_img)  # Read image\n",
                "    # Defining all the parameters\n",
                "    t_lower = 100  # Lower Threshold\n",
                "    t_upper = 200  # Upper threshold\n",
                "    aperture_size = 5  # Aperture size\n",
                "    L2Gradient = True  # Boolean\n",
                "\n",
                "    # Applying the Canny Edge filter\n",
                "    # with Aperture Size and L2Gradient\n",
                "    edge = cv2.Canny(img, t_lower, t_upper,\n",
                "                     apertureSize=aperture_size,\n",
                "                     L2gradient=L2Gradient)\n",
                "    return cv2.imwrite(fpath_img, edge)\n",
                "    \"\"\" return cv2.imwrite(str(i)+'_canny_'+fpath_img, edge) \"\"\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "image = tf.keras.preprocessing.image.load_img(\n",
                "            fpath, target_size=(224, 224))\n",
                "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
                "        image = (image - 128.) / 128."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [{
                "name": "stdout",
                "output_type": "stream",
                "text": [
                    "0       A\n",
                    "1       A\n",
                    "2       A\n",
                    "3       A\n",
                    "4       A\n",
                    "       ..\n",
                    "5434    Y\n",
                    "5435    Y\n",
                    "5436    Y\n",
                    "5437    Y\n",
                    "5438    Y\n",
                    "Name: labels, Length: 5439, dtype: object\n"
                ]
            }],
            "source": [
                "array_label = os.listdir(data_dir)\n",
                "count_lbl = len(array_label)\n",
                "filepaths = []\n",
                "labels = []\n",
                "for lbl in array_label:\n",
                "    classpath = os.path.join(data_dir, lbl)\n",
                "    flist = os.listdir(classpath)\n",
                "    for f, i in zip(flist, range(len(flist))):\n",
                "        fpath = os.path.join(classpath, f)\n",
                "        labels.append(lbl)\n",
                "        if create_filter:\n",
                "            filepaths.append(create_filter_canny(fpath, i))\n",
                "        else:\n",
                "            filepaths.append(fpath)\n",
                "\n",
                "Fseries = pd.Series(filepaths, name='filepaths')\n",
                "Lseries = pd.Series(labels, name='labels')\n",
                "df = pd.concat([Fseries, Lseries], axis=1)\n",
                "print(df['labels'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_split_tvt():\n",
                "    parent_path_file = Path(str(path_file)).parent\n",
                "    exist_csv_with_split = os.path.isfile(\n",
                "        (str(parent_path_file) + '/' + str(csv_name_split_train)))\n",
                "    if not exist_csv_with_split:\n",
                "        stratify_use = df['labels']\n",
                "        tamano_df = len(df)\n",
                "        train_size_parameter = .70\n",
                "        test_size_parameter = .2\n",
                "        strat = df['labels']\n",
                "\n",
                "        train_df, residue_df = train_test_split(\n",
                "            df, train_size=train_size_parameter, shuffle=True, random_state=123, stratify=strat)\n",
                "\n",
                "        strat_residue_df = residue_df['labels']\n",
                "        test_size_parameter_use = (float(tamano_df) * test_size_parameter)/1\n",
                "        train_size_test = (test_size_parameter_use*1)/len(strat_residue_df)\n",
                "        valid_df, test_df = train_test_split(\n",
                "            residue_df, train_size=train_size_test, shuffle=True, random_state=123, stratify=strat_residue_df)\n",
                "\n",
                "        print('train_df length: ', len(train_df), '  valid_df length: ',\n",
                "              len(valid_df), '  test_df length: ', len(test_df))\n",
                "        # print(train_df)\n",
                "        print('\\nBalance de los datos de entrenamiento: ')\n",
                "        print(train_df['labels'].value_counts(ascending=True))\n",
                "\n",
                "        if os.path.isdir(path_file):\n",
                "            print('La carpeta existe.')\n",
                "        else:\n",
                "            os.mkdir(path_file)\n",
                "        train_save_split = os.path.join(parent_path_file, csv_name_split_train)\n",
                "        train_df.to_csv(train_save_split, index=False)\n",
                "\n",
                "        val_save_split = os.path.join(parent_path_file, csv_name_split_val)\n",
                "        valid_df.to_csv(val_save_split, index=False)\n",
                "\n",
                "        test_save_split = os.path.join(parent_path_file, csv_name_split_test)\n",
                "        test_df.to_csv(test_save_split, index=False)\n",
                "\n",
                "        split_train = pd.read_csv(train_save_split)\n",
                "        split_val = pd.read_csv(val_save_split)\n",
                "        split_test = pd.read_csv(test_save_split)\n",
                "        return split_train, split_val, split_test\n",
                "    else:\n",
                "        train_save_split = os.path.join(parent_path_file, csv_name_split_train)\n",
                "        val_save_split = os.path.join(parent_path_file, csv_name_split_val)\n",
                "        test_save_split = os.path.join(parent_path_file, csv_name_split_test)\n",
                "        split_train = pd.read_csv(train_save_split)\n",
                "        split_val = pd.read_csv(val_save_split)\n",
                "        split_test = pd.read_csv(test_save_split)\n",
                "        return split_train, split_val, split_test\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [{
                "name": "stdout",
                "output_type": "stream",
                "text": [
                    "H    174\n",
                    "E    187\n",
                    "F    232\n",
                    "P    249\n",
                    "Y    250\n",
                    "B    250\n",
                    "V    250\n",
                    "T    250\n",
                    "Q    250\n",
                    "L    250\n",
                    "N    250\n",
                    "A    251\n",
                    "K    252\n",
                    "U    252\n",
                    "X    252\n",
                    "O    253\n",
                    "M    255\n",
                    "W    256\n",
                    "D    259\n",
                    "I    260\n",
                    "C    275\n",
                    "R    282\n",
                    "Name: labels, dtype: int64\n"
                ]
            }],
            "source": [
                "print(df.labels.value_counts(ascending=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [{
                "ename": "ParserError",
                "evalue": "Error tokenizing data. C error: Expected 1 fields in line 27, saw 411\n",
                "output_type": "error",
                "traceback": [
                    "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                    "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
                    "\u001b[1;32mc:\\wamp64\\www\\LSC\\Model_CNN_Preentreno_Funcional12_1.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000010?line=0'>1</a>\u001b[0m train_df, valid_df, test_df \u001b[39m=\u001b[39m create_split_tvt()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000010?line=1'>2</a>\u001b[0m \u001b[39m\"\"\" prueba =dict(df.labels.value_counts(ascending=True))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000010?line=2'>3</a>\u001b[0m \u001b[39mfor k,v in sorted(prueba.items()):\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000010?line=3'>4</a>\u001b[0m \u001b[39m    print(k,v-174) \"\"\"\u001b[39;00m\n",
                    "\u001b[1;32mc:\\wamp64\\www\\LSC\\Model_CNN_Preentreno_Funcional12_1.ipynb Cell 9'\u001b[0m in \u001b[0;36mcreate_split_tvt\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000008?line=46'>47</a>\u001b[0m test_save_split \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(parent_path_file, csv_name_split_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000008?line=47'>48</a>\u001b[0m split_train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(train_save_split)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000008?line=48'>49</a>\u001b[0m split_val \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(val_save_split)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000008?line=49'>50</a>\u001b[0m split_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(test_save_split)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000008?line=50'>51</a>\u001b[0m \u001b[39mreturn\u001b[39;00m split_train, split_val, split_test\n",
                    "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
                    "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=664'>665</a>\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=665'>666</a>\u001b[0m     dialect,\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=666'>667</a>\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=675'>676</a>\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=676'>677</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=677'>678</a>\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=679'>680</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
                    "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=577'>578</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=579'>580</a>\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=580'>581</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
                    "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=1251'>1252</a>\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=1252'>1253</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=1253'>1254</a>\u001b[0m     index, columns, col_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(nrows)\n\u001b[0;32m   <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=1254'>1255</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=1255'>1256</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
                    "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=222'>223</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=223'>224</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=224'>225</a>\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=225'>226</a>\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=226'>227</a>\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
                    "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
                    "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
                    "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
                    "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
                    "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 27, saw 411\n"
                ]
            }],
            "source": [
                "train_df, valid_df, test_df = create_split_tvt()\n",
                "\"\"\" prueba =dict(df.labels.value_counts(ascending=True))\n",
                "for k,v in sorted(prueba.items()):\n",
                "    print(k,v-174) \"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def data_aumentation_gen(train_df, train_df_datagen, flag):\n",
                "    if flag == False:\n",
                "        train_df_shuffle = sklearn.utils.shuffle(train_df)\n",
                "        new_filepaths = []\n",
                "        new_labels = []\n",
                "        i = 0\n",
                "        num_images = 0\n",
                "        balance_train_df = dict(train_df.labels.value_counts(ascending=True))\n",
                "        max_item = max(balance_train_df.values())\n",
                "        count_img = 1\n",
                "        val = 0\n",
                "        print(max_item)\n",
                "        for k, v in balance_train_df.items():\n",
                "            for imgs_path, lbl in train_df_shuffle.values:\n",
                "                if k == lbl:\n",
                "                    if (count_img+v <= max_item):\n",
                "                        img = os.path.join(imgs_path)\n",
                "                        imge = load_img(img)\n",
                "                        imge = cv2.resize(image.img_to_array(imge), img_size,\n",
                "                                          interpolation=cv2.INTER_AREA)\n",
                "                        x = imge/255\n",
                "                        x = np.expand_dims(x, axis=0)\n",
                "                        for output_batch in train_df_datagen.flow(x, batch_size=1):\n",
                "                            a = image.img_to_array(output_batch[0])\n",
                "                            imagen = output_batch[0, :, :]*255\n",
                "                            imgfinal = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)\n",
                "                            img_save = (str(Path(imgs_path).parent)+\"/\"+lbl+\"_balance_%i%i.jpg\" %\n",
                "                                        (i, num_images))\n",
                "                            cv2.imwrite(img_save, imgfinal)\n",
                "                            num_images += 1\n",
                "                            new_filepaths.append(img_save)\n",
                "                            new_labels.append(lbl)\n",
                "                            count_img += 1\n",
                "                            break\n",
                "                    else:\n",
                "                        print(str(k)+' '+str(count_img-1))\n",
                "                        count_img = 1\n",
                "                        break\n",
                "            i += 1\n",
                "        Fseries_ = pd.Series(new_filepaths, name='filepaths')\n",
                "        Lseries_ = pd.Series(new_labels, name='labels')\n",
                "        train_df_temp = pd.concat([Fseries_, Lseries_], axis=1)\n",
                "        \"\"\" print(\"images generated\", num_images) \"\"\"\n",
                "        flag = True\n",
                "        return train_df.append(train_df_temp), flag\n",
                "    else:\n",
                "        return train_df, flag\n",
                "\n",
                "\n",
                "train_df, flag = data_aumentation_gen(train_df, train_df_datagen, flag)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# color_mode='grayscale',\n",
                "img_train_gen = train_df_datagen.flow_from_dataframe(\n",
                "    train_df,\n",
                "    x_col='filepaths',\n",
                "    y_col='labels',\n",
                "    target_size=img_size,\n",
                "    batch_size=batch_size,\n",
                "    color_mode='rgb',\n",
                "    shuffle=True,\n",
                "    class_mode='categorical')\n",
                "img_val_gen = valid_df_datagen.flow_from_dataframe(\n",
                "    valid_df,\n",
                "    x_col='filepaths',\n",
                "    y_col='labels',\n",
                "    target_size=img_size,\n",
                "    batch_size=batch_size,\n",
                "    color_mode='rgb',\n",
                "    shuffle=False,\n",
                "    class_mode='categorical')\n",
                "img_test_gen = test_df_datagen.flow_from_dataframe(\n",
                "    test_df,\n",
                "    x_col='filepaths',\n",
                "    y_col='labels',\n",
                "    target_size=img_size,\n",
                "    batch_size=batch_size,\n",
                "    color_mode='rgb',\n",
                "    shuffle=False,\n",
                "    class_mode='categorical')\n",
                "\n",
                "classes = list(img_train_gen.class_indices.keys())\n",
                "print(classes)\n",
                "class_count = len(classes)\n",
                "train_steps = np.ceil(img_train_gen.n/batch_size)\n",
                "val_steps = np.ceil(img_val_gen.n/batch_size)\n",
                "test_steps = np.ceil(img_test_gen.n/batch_size)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def show_image_samples(gen):\n",
                "    t_dict = gen.class_indices\n",
                "    classes = list(t_dict.keys())\n",
                "    images, labels = next(gen)  # get a sample batch from the generator\n",
                "    plt.figure(figsize=(15, 15))\n",
                "    length = len(labels)\n",
                "    if length < 25:  # show maximum of 25 images\n",
                "        r = length\n",
                "    else:\n",
                "        r = 25\n",
                "    for i in range(r):\n",
                "        plt.subplot(5, 5, i + 1)\n",
                "        image = images[i]/255\n",
                "        plt.imshow(image)\n",
                "        index = np.argmax(labels[i])\n",
                "        class_name = classes[index]\n",
                "        plt.title(class_name, color='blue', fontsize=12)\n",
                "        plt.axis('off')\n",
                "    plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "show_image_samples(img_train_gen)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "cnn_base = tf.keras.applications.EfficientNetV2S(\n",
                "    include_top=False,\n",
                "    weights='imagenet',\n",
                "    input_shape=img_shape,\n",
                "    pooling='max',\n",
                ")\n",
                "x = cnn_base.output\n",
                "x = keras.layers.Flatten()(x)\n",
                "x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n",
                "x = Dense(256, kernel_regularizer=regularizers.l2(l=0.016), activity_regularizer=regularizers.l1(0.006),\n",
                "          bias_regularizer=regularizers.l1(0.006), activation='relu')(x)\n",
                "x = Dropout(rate=.45, seed=123)(x)\n",
                "output = Dense(class_count, activation='softmax')(x)\n",
                "model = Model(inputs=cnn_base.input, outputs=output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "sgd = optimizers.SGD(lr=1E-2, momentum=0.91,decay=5**(-4), nesterov=True)\n",
                "Y ADAM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.compile(loss='categorical_crossentropy',\n",
                "              optimizer=Adam(), metrics=['accuracy'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def my_learning_rate(epoch, lrate):\n",
                "    return lrate\n",
                "\n",
                "\n",
                "lrs = LearningRateScheduler(my_learning_rate)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logdir = os.path.join(\n",
                "    path_file, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
                "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
                "cnn_fit = model.fit(img_train_gen,\n",
                "                    steps_per_epoch=train_steps,\n",
                "                    epochs=epoch,\n",
                "                    validation_data=img_val_gen,\n",
                "                    validation_steps=test_steps,\n",
                "                    callbacks=[tensorboard_callback, lrs],\n",
                "                    verbose=1,\n",
                "                    initial_epoch=0)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd_model_history = pd.DataFrame(cnn_fit.history)\n",
                "log_name = str(\n",
                "    'log-'+datestring+'.xlsx')\n",
                "path_classification_report_csv = os.path.join(\n",
                "    path_file, log_name)\n",
                "pd_model_history.to_excel(\n",
                "    path_classification_report_csv, index=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bal = train_df['labels'].value_counts()\n",
                "fig, ax = plt.subplots(figsize=(16, 9))\n",
                "# Creamos Gráfica y ponesmos las barras de color verde\n",
                "  # oculta la cuadricula\n",
                "bar_train =ax.barh(bal.index, bal.values)\n",
                "ax.xaxis.set_tick_params(pad=5)\n",
                "ax.yaxis.set_tick_params(pad=10)\n",
                "plt.ylabel('Letters')\n",
                "plt.xlabel('Quantity')\n",
                "ax.bar_label(bar_train, padding=5)\n",
                "plt.title('Balance Train Dataset')\n",
                "plt.savefig(os.path.join(path_file, 'Balance_Train_Dataset.jpg'),\n",
                "                            dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "%tensorboard --logdir 'c:/Users/AnonimusXD/Documents/Model Proyecto de grado/saved_model'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "acc = cnn_fit.history['accuracy']\n",
                "val_acc = cnn_fit.history['val_accuracy']\n",
                "\n",
                "loss = cnn_fit.history['loss']\n",
                "val_loss = cnn_fit.history['val_loss']\n",
                "\n",
                "epochs_range = range(epoch)\n",
                "\n",
                "plt.figure(figsize=(8, 8))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
                "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
                "plt.legend(loc='lower right')\n",
                "plt.title('Training and Validation Accuracy')\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(epochs_range, loss, label='Training Loss')\n",
                "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
                "plt.legend(loc='upper right')\n",
                "plt.title('Training and Validation Loss')\n",
                "plt.savefig(os.path.join(path_file, 'TA-VA-TL-VL.jpg'))\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate generalization metrics\n",
                "score = model.evaluate(img_test_gen, verbose=1)\n",
                "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Save the model\n",
                "name_model_save = str('model-'+model_name+'-'+datestring+'.h5')\n",
                "if os.path.isdir(path_file):\n",
                "    print('La carpeta existe.')\n",
                "else:\n",
                "    os.mkdir(path_file)\n",
                "filefolder_model = os.path.join(path_file, name_model_save)\n",
                "model.save(filefolder_model)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Save dataframe validation\n",
                "name_csv_df_val = str('temp-path-img-'+model_name+'-'+datestring+'.csv')\n",
                "filefolder_dfv = os.path.join(path_file, name_csv_df_val)\n",
                "test_df.to_csv(filefolder_dfv, encoding='utf-8', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create csv with classes\n",
                "Class_series = pd.Series(classes, name='class')\n",
                "class_df = pd.concat([Class_series], axis=1)\n",
                "csv_name = 'class.csv'\n",
                "csv_save_class = os.path.join(path_file, csv_name)\n",
                "class_df.to_csv(csv_save_class, index=True, index_label='index')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = model.predict(img_test_gen, verbose=1)\n",
                "print_code = 0\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_in_color(txt_msg, fore_tupple, back_tupple,):\n",
                "    # prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple\n",
                "    # text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n",
                "    rf, gf, bf = fore_tupple\n",
                "    rb, gb, bb = back_tupple\n",
                "    msg = '{0}' + txt_msg\n",
                "    mat = '\\33[38;2;' + str(rf) + ';' + str(gf) + ';' + str(bf) + \\\n",
                "        ';48;2;' + str(rb) + ';' + str(gb) + ';' + str(bb) + 'm'\n",
                "    print(msg .format(mat), flush=True)\n",
                "    print('\\33[0m', flush=True)  # returns default print color to back to black\n",
                "    return\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_ci(cm, param, alpha=0.05, method=\"normal-approx\"):\n",
                "    \"\"\"\n",
                "    Plot two-sided confidence interval.\n",
                "\n",
                "    :param cm: ConfusionMatrix\n",
                "    :type cm : pycm.ConfusionMatrix object\n",
                "    :param param: input parameter\n",
                "    :type param: str\n",
                "    :param alpha: type I error\n",
                "    :type alpha: float\n",
                "    :param method: binomial confidence intervals method\n",
                "    :type method: str\n",
                "    :return: None\n",
                "    \"\"\"\n",
                "    conf_str = str(round(100*(1-alpha)))\n",
                "    print(conf_str+\"%CI :\")\n",
                "    if param in cm.class_stat.keys():\n",
                "        mean = []\n",
                "        error = [[], []]\n",
                "        data = cm.CI(param, alpha=alpha, binom_method=method)\n",
                "        class_names_str = list(map(str, (cm.classes)))\n",
                "        for class_index, class_name in enumerate(cm.classes):\n",
                "            \"\"\" print(str(class_name)+\" : \"+str(data[class_name][1])) \"\"\"\n",
                "            mean.append(cm.class_stat[param][class_name])\n",
                "            error[0].append(cm.class_stat[param][class_name] -\n",
                "                            data[class_name][1][0])\n",
                "            error[1].append(data[class_name][1][1] -\n",
                "                            cm.class_stat[param][class_name])\n",
                "        fig, ax = plt.subplots(figsize=(10, 22))\n",
                "        \"\"\" fig = plt.figure() \"\"\"\n",
                "        plt.errorbar(mean, class_names_str, xerr=error,\n",
                "                     fmt='o', capsize=5, linestyle=\"dotted\")\n",
                "        plt.ylabel('Class')\n",
                "        fig.suptitle(\"Param :\"+param + \", Alpha:\"+str(alpha), fontsize=16)\n",
                "        for index, value in enumerate(mean):\n",
                "            down_point = data[cm.classes[index]][1][0]\n",
                "            up_point = data[cm.classes[index]][1][1]\n",
                "            plt.text(value, class_names_str[index], \"%f\" %\n",
                "                     value, ha=\"center\", va=\"top\", color=\"red\")\n",
                "            plt.text(down_point, class_names_str[index], \"%f\" %\n",
                "                     down_point, ha=\"right\", va=\"bottom\", color=\"red\")\n",
                "            plt.text(up_point, class_names_str[index], \"%f\" %\n",
                "                     up_point, ha=\"left\", va=\"bottom\", color=\"red\")\n",
                "    else:\n",
                "        mean = cm.overall_stat[param]\n",
                "        data = cm.CI(param, alpha=alpha, binom_method=method)\n",
                "        print(data[1])\n",
                "        error = [[], []]\n",
                "        up_point = data[1][1]\n",
                "        down_point = data[1][0]\n",
                "        error[0] = [cm.overall_stat[param] - down_point]\n",
                "        error[1] = [up_point - cm.overall_stat[param]]\n",
                "        fig, ax = plt.subplots(figsize=(10, 22))\n",
                "        \"\"\" fig = plt.figure() \"\"\"\n",
                "        plt.errorbar(mean, [param], xerr=error, fmt='o',\n",
                "                     capsize=5, linestyle=\"dotted\")\n",
                "        fig.suptitle(\"Alpha:\"+str(alpha), fontsize=16)\n",
                "        plt.text(mean, param, \"%f\" % mean, ha=\"center\", va=\"top\", color=\"red\")\n",
                "        plt.text(down_point, param, \"%f\" % down_point,\n",
                "                 ha=\"right\", va=\"bottom\", color=\"red\")\n",
                "        plt.text(up_point, param, \"%f\" %\n",
                "                 up_point, ha=\"left\", va=\"bottom\", color=\"red\")\n",
                "\n",
                "    plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def multiclass_roc_auc_score(y_test, y_pred, classes, average=\"macro\"):\n",
                "    fig, c_ax = plt.subplots(1, 1, figsize=(12, 8))\n",
                "    lb = LabelBinarizer()\n",
                "    lb.fit(y_test)\n",
                "    y_test = lb.transform(y_test)\n",
                "    y_pred = lb.transform(y_pred)\n",
                "\n",
                "    for (idx, c_label) in enumerate(classes):\n",
                "        fpr, tpr, thresholds = roc_curve(\n",
                "            y_test[:, idx].astype(int), y_pred[:, idx])\n",
                "        c_ax.plot(fpr, tpr, label='%s (AUC:%0.3f)' % (c_label, auc(fpr, tpr)))\n",
                "    c_ax.plot(fpr, fpr, 'k--', label='Random Guessing')\n",
                "    c_ax.annotate('Random Guess', (.5, .48), color='black')\n",
                "    c_ax.legend()\n",
                "    c_ax.set_xlabel('False Positive Rate')\n",
                "    c_ax.set_ylabel('True Positive Rate')\n",
                "    plt.savefig(os.path.join(path_file, 'multiclass_roc_auc_score.jpg'),\n",
                "                dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
                "    plt.show()\n",
                "    print('ROC AUC score:', roc_auc_score(y_test, y_pred, average=average))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_info(test_gen, preds):\n",
                "    class_dict = test_gen.class_indices\n",
                "    labels = test_gen.labels\n",
                "    file_names = test_gen.filenames\n",
                "    error_list = []\n",
                "    true_class = []\n",
                "    pred_class = []\n",
                "    prob_list = []\n",
                "    new_dict = {}\n",
                "    error_indices = []\n",
                "    y_pred = []\n",
                "    for key, value in class_dict.items():\n",
                "        # dictionary {integer of class number: string of class name}\n",
                "        new_dict[value] = key\n",
                "    # store new_dict as a text fine in the save_dir\n",
                "    classes = list(new_dict.values())     # list of string of class names\n",
                "    errors = 0\n",
                "    # False Negative\n",
                "    x_predic = []\n",
                "    for i, p in enumerate(preds):\n",
                "        pred_index = np.argmax(p)\n",
                "        x_predic.append(pred_index)\n",
                "        true_index = labels[i]  # labels are integer values\n",
                "        if pred_index != true_index:  # a misclassification has occurred\n",
                "            error_list.append(file_names[i])\n",
                "            true_class.append(new_dict[true_index])\n",
                "            pred_class.append(new_dict[pred_index])\n",
                "            prob_list.append(p[pred_index])\n",
                "            error_indices.append(true_index)\n",
                "            errors = errors + 1\n",
                "        y_pred.append(pred_index)\n",
                "\n",
                "    y_true = np.array(labels)\n",
                "    y_pred = np.array(y_pred)\n",
                "    if len(classes) <= 30:\n",
                "        # create a confusion matrix\n",
                "        cm2 = ConfusionMatrix(actual_vector=y_true, predict_vector=y_pred)\n",
                "        class_stat = pd.DataFrame(cm2.class_stat).transpose()\n",
                "        class_stat_csv = str(\n",
                "            'class_stat_'+datestring+'.xlsx')\n",
                "        class_stat_report_csv = os.path.join(\n",
                "            path_file, class_stat_csv)\n",
                "        class_stat.to_excel(\n",
                "            class_stat_report_csv, index=True)\n",
                "\n",
                "        fn_cm = cm2.FN\n",
                "        fp_cm = cm2.FP\n",
                "        tn_cm = cm2.TN\n",
                "        tp_cm = cm2.TP\n",
                "        count_fp = 0\n",
                "        count_fn = 0\n",
                "        count_tn = 0\n",
                "        count_tp = 0\n",
                "\n",
                "        fig, ax = plt.subplots(figsize=(10, 2))\n",
                "        for k, v in fn_cm.items():\n",
                "            if v > 0:\n",
                "                bar_char_fn = ax.barh(classes[k], v, color='gray')\n",
                "                count_fn += v\n",
                "                plt.ylabel('Letters')\n",
                "                plt.xlabel('Quantity')\n",
                "                plt.title('False Negative = '+str(count_fn))\n",
                "                ax.bar_label(bar_char_fn, padding=5)\n",
                "                plt.savefig(os.path.join(path_file, 'False Negative.jpg'),\n",
                "                            dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
                "\n",
                "        fig, ax = plt.subplots(figsize=(10, 2))\n",
                "        for k, v in fp_cm.items():\n",
                "            if v > 0:\n",
                "                bar_char_fp = ax.barh(classes[k], v, color='gray')\n",
                "                count_fp += v\n",
                "                plt.ylabel('Letters')\n",
                "                plt.xlabel('Quantity')\n",
                "                plt.title('False Positive = '+str(count_fp))\n",
                "                ax.bar_label(bar_char_fp, padding=5)\n",
                "                plt.savefig(os.path.join(path_file, 'False Positive.jpg'),\n",
                "                            dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
                "\n",
                "        fig, ax = plt.subplots(figsize=(10, 10))\n",
                "        for k, v in tn_cm.items():\n",
                "            if v > 0:\n",
                "                bar_char_tn = ax.barh(classes[k], v, color='gray')\n",
                "                count_tn += v\n",
                "                plt.ylabel('Letters')\n",
                "                plt.xlabel('Quantity')\n",
                "                plt.title('True Negative = '+str(count_tn))\n",
                "                ax.bar_label(bar_char_tn, padding=5)\n",
                "                plt.savefig(os.path.join(path_file, 'True Negative.jpg'),\n",
                "                            dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
                "        fig, ax = plt.subplots(figsize=(10, 10))\n",
                "        for k, v in tp_cm.items():\n",
                "            if v > 0:\n",
                "                bar_char_tp = ax.barh(classes[k], v, color='gray')\n",
                "                count_tp += v\n",
                "                plt.ylabel('Letters')\n",
                "                plt.xlabel('Quantity')\n",
                "                plt.title('True Positive = '+str(count_tp))\n",
                "                ax.bar_label(bar_char_tp, padding=5)\n",
                "                plt.savefig(os.path.join(path_file, 'True Positive.jpg'),\n",
                "                            dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
                "\n",
                "        cm = confusion_matrix(y_true, y_pred)\n",
                "        length = len(classes)\n",
                "        if length < 8:\n",
                "            fig_width = 8\n",
                "            fig_height = 8\n",
                "        else:\n",
                "            fig_width = int(length * .5)\n",
                "            fig_height = int(length * .5)\n",
                "        plt.figure(figsize=(fig_width, fig_height))\n",
                "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues')\n",
                "        plt.xticks(np.arange(length)+.5, classes, rotation=90)\n",
                "        plt.yticks(np.arange(length)+.5, classes, rotation=0)\n",
                "        plt.xlabel(\"Predicted\")\n",
                "        plt.ylabel(\"True Class\")\n",
                "        plt.title(\"Confusion Matrix\")\n",
                "        plt.savefig(os.path.join(path_file, 'confusion_matrix.jpg'),\n",
                "                    dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
                "        plt.show()\n",
                "    clr = classification_report(\n",
                "        y_true, y_pred, target_names=classes, output_dict=True)\n",
                "\n",
                "    df_classification_report = pd.DataFrame(clr).transpose()\n",
                "\n",
                "    name_classification_report_csv = str(\n",
                "        'classification_report-'+datestring+'.xlsx')\n",
                "    path_classification_report_csv = os.path.join(\n",
                "        path_file, name_classification_report_csv)\n",
                "    df_classification_report.to_excel(\n",
                "        path_classification_report_csv, index=True)\n",
                "    \"\"\" print(\"Classification Report:\\n----------------------\\n\",\n",
                "          df_classification_report) \"\"\"\n",
                "    multiclass_roc_auc_score(y_true, x_predic, classes)\n",
                "\n",
                "\n",
                "print_info(img_test_gen, predicted)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_image(predictions_array, class_df, true_label, img, i):\n",
                "    predictions_array, true_label = predictions_array, true_label\n",
                "    plt.figure(figsize=(20, 6))\n",
                "    grid = plt.GridSpec(1, 5, hspace=0.2, wspace=0.2)\n",
                "    plt.subplot(grid[0, 0])\n",
                "    plt.grid(b=None)\n",
                "    plt.imshow(img)\n",
                "    predicted_label = class_df.iloc[np.argmax(predictions_array)]\n",
                "    if predicted_label == true_label:\n",
                "        color = 'green'\n",
                "    else:\n",
                "        color = 'red'\n",
                "    plt.xlabel(\"{} {:6.2f}% ({})\".format(predicted_label,\n",
                "                                         100*np.max(predictions_array),\n",
                "                                         true_label),\n",
                "               color=color)\n",
                "    plt.subplot(grid[0, 1:])\n",
                "    plt.xticks(range(len(classes)), classes)\n",
                "    thisplot = plt.bar(range(len(classes)), predictions_array, color=\"#777777\")\n",
                "    ax.bar_label(thisplot, label_type='center')\n",
                "    plt.ylim([0, 1])\n",
                "    p_label = np.argmax(predictions_array)\n",
                "    thisplot[p_label].set_color('red')\n",
                "    thisplot[classes.index(true_label)].set_color('green')\n",
                "    i = 0\n",
                "    for p in thisplot:\n",
                "        width = p.get_width()\n",
                "        height = p.get_height()\n",
                "        x, y = p.get_xy()\n",
                "        plt.text(x+width/2,\n",
                "                 y+height*1.01,\n",
                "                 str(round(predictions_array[i]*100, 2))+'%',\n",
                "                 ha='center',\n",
                "                 weight='bold')\n",
                "        i += 1\n",
                "    if i < 5:\n",
                "        plt.savefig(os.path.join(path_file, str(i)+'_'+str(true_label)+'_predict_test.jpg'),\n",
                "                    dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
                "    plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def classify_with_model(i_, class_csv_path, filefolder_val_img,  model_path):\n",
                "    # read in the csv file\n",
                "    class_df = pd.read_csv(class_csv_path)\n",
                "    images_csv = pd.read_csv(filefolder_val_img)\n",
                "    crop_image = False\n",
                "    img_height = 224\n",
                "    img_width = 224\n",
                "    img_size_ = (img_width, img_height)\n",
                "    scale = 1\n",
                "    try:\n",
                "        s = int(scale)\n",
                "        s2 = 1\n",
                "        s1 = 0\n",
                "    except:\n",
                "        split = scale.split('-')\n",
                "        s1 = float(split[1])\n",
                "        s2 = float(split[0].split('*')[1])\n",
                "        print(s1, s2)\n",
                "    model = load_model(model_path)\n",
                "    for i in range(i_):\n",
                "        rand = random.randint(0, len(images_csv['filepaths']))\n",
                "        image_path = images_csv['filepaths'].iloc[rand]\n",
                "        class_img_path = images_csv['labels'].iloc[rand]\n",
                "        img = plt.imread(image_path)\n",
                "        img = cv2.resize(img, img_size_)\n",
                "        img_use = img\n",
                "        img = img*s2 - s1\n",
                "        img = np.expand_dims(img, axis=0)\n",
                "        p = np.squeeze(model.predict(img))\n",
                "        plot_image(p, class_df['class'], class_img_path, img_use, i)\n",
                "        plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "\n",
                "\n",
                "def create_galery_val():\n",
                "    path_imgs = filefolder_dfv\n",
                "    images_csv = pd.read_csv(path_imgs)\n",
                "    filepath_img = []\n",
                "    labels_img = []\n",
                "    store_path = os.path.join(\n",
                "        path_file, 'storage-img-'+model_name+'-'+datestring)\n",
                "    os.mkdir(store_path)\n",
                "    for i in range(len(images_csv['filepaths'])):\n",
                "        image_path = images_csv['filepaths'].iloc[i]\n",
                "        class_img_path = images_csv['labels'].iloc[i]\n",
                "        img = cv2.imread(image_path)\n",
                "        file_name = os.path.split(image_path)[1]\n",
                "        dst_path = os.path.join(store_path, file_name)\n",
                "        filepath_img.append(dst_path)\n",
                "        labels_img.append(class_img_path)\n",
                "        cv2.imwrite(dst_path, img)\n",
                "    Fseries = pd.Series(filepath_img, name='filepaths')\n",
                "    Lseries = pd.Series(labels_img, name='labels')\n",
                "    df_img_val_new_file = pd.concat([Fseries, Lseries], axis=1)\n",
                "    name_csv_df_val = str('new-'+model_name+'-'+datestring+'.csv')\n",
                "    filefolder_val_img = os.path.join(path_file, name_csv_df_val)\n",
                "    df_img_val_new_file.to_csv(\n",
                "        filefolder_val_img, encoding='utf-8', index=False)\n",
                "    return store_path, filefolder_val_img\n",
                "\n",
                "\n",
                "store_path_img, filefolder_val_img = create_galery_val()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save all path\n",
                "data_paths = {'model': [filefolder_model],\n",
                "              'class': [csv_save_class], 'img_val': [filefolder_val_img]}\n",
                "\n",
                "origin_paths = pd.DataFrame(data_paths)\n",
                "csv_name = 'origin_path.csv'\n",
                "csv_save_loc = os.path.join(path_file, csv_name)\n",
                "origin_paths.to_csv(csv_save_loc, index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "\n",
                "path_imgs = store_path_img\n",
                "class_csv_path = csv_save_class\n",
                "model_path = filefolder_model\n",
                "num_images = 5\n",
                "classify_with_model(num_images, class_csv_path,\n",
                "                    filefolder_val_img, model_path)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.4 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "2930d103325ed71c6e2381c6daf3a88f175707bf3585d18173a8236fe5d58e8b"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
